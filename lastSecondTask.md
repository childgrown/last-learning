# 误差
误差：样本真实输出与预测输出之间的差异

- 训练（经验）误差：训练集上

- 测试无常：测试集

- 泛化误差：除训练集以外所有样本

## 过拟合（over-fitting）：
学习器把学的太好了，已经把训练样本的一些特点当作所有潜在样本的都会有的一般性质，这样就会导致泛化性能下降。通常是由于学习能力低下造成的，欠拟合比较容易克服，例如
- 在决策树学习中扩展分支
- 在神经网络学习中增加训练轮数等
- 寻找更好的特征-----具有代表性的
- 用更多的特征-----增大输入向量的维度

## 欠拟合（under-fitting）：
对训练样本的一般性质都尚未学习好,通常采用
- 正则化-----即在对模型的目标函数（objective function）或代价函数（cost function）加上正则项
- 增大数据集合-----使用更多的数据
- 噪声点比重减少、减少数据特征-----减小数据维度，高维空间密度小
- 交叉验证法

# 协变量偏移、标签偏移、概念偏移
## 协变量偏移
统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。

- 简单理解是指输入的训练集的数据类型和测试数据集的数据类型完全不一致。会造成欠拟合的情况。

## 标签偏移
当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。

病因（要预测的诊断结果）导致 症状（观察到的结果）。

训练数据集，数据很少只包含流感p(y)的样本。

而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。

- 简单理解就是测试数据集类型多于训练数据集。

## 概念迁移
标签本身的定义发生变化的情况
- 简单理解是指对于同一事物的概念以及描述产生变化，而对实际预测有影响。比如可乐、苏打水、汽水，都指同一事物，但是不同地区表示的描述不同。

# 交叉验证方法
## 留一交叉验证（leave-one-out）：
每次从个数为N的样本集中，取出一个样本作为验证集，剩下的N-1个作为训练集，重复进行N次。最后平均N个结果作为泛化误差估计。
## 留P交叉验证（leave-P-out）：
与留一类似，但是每次留P个样本。每次从个数为N的样本集中，取出P个样本作为验证集，剩下的N-P个作为训练集，重复进行Cn取p次。最后平均N个结果作为泛化误差估计。
## K折交叉验证
所谓K折交叉验证，就是将数据集等比例划分成K份，以其中的一份作为测试数据，其他的K-1份数据作为训练数据。然后，这样算是一次实验，而K折交叉验证只有实验K次才算完成完整的一次，也就是说交叉验证实际是把实验重复做了K次，每次实验都是从K个部分选取一份不同的数据部分作为测试数据（保证K个部分的数据都分别做过测试数据），剩下的K-1个当作训练数据，最后把得到的K个实验结果进行平分。


